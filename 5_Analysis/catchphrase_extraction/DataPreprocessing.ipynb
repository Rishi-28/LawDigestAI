{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Data Preprocessing - Sentence Token Reduction"],"metadata":{"id":"X6twg3IiJ72h"}},{"cell_type":"code","source":["# Global Variables\n","DRIVE_HOME = '/content/drive'\n","CODE_HOME = '/MyDrive/LawDigestAI'\n","\n","# Drive Mount\n","from google.colab import drive\n","drive.mount(DRIVE_HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQx-UPhmgYxb","outputId":"0059a2ef-cd26-4fcc-f83b-ec4ad12756e0","executionInfo":{"status":"ok","timestamp":1732401945968,"user_tz":300,"elapsed":1277,"user":{"displayName":"Nithish Kumar","userId":"11511991541260541160"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### Pipeline\n","- invoke the input file (which has sentences exceeding token limit(512))\n","- Using sentence transformers, generate embeddings for both catchphrases and sentences for each row\n","- compute cosine similarity between catchphrase embeddings and sentences embeddings\n","- Now, loop over for each sentence in the matrix and take the top similarity and append to a list. Sort it in decreasing order.\n","- check and truncate the rest of the sentence that exceeds the max token limit (512)"],"metadata":{"id":"lvyMydA5KHGh"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb5P7NEgpDS4","outputId":"bbb56dc9-2a17-4bed-b9be-95069e2b84d6","executionInfo":{"status":"ok","timestamp":1732401947541,"user_tz":300,"elapsed":1574,"user":{"displayName":"Nithish Kumar","userId":"11511991541260541160"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["\n","import pandas as pd\n","\n","import nltk\n","nltk.download('punkt_tab')\n","\n","from sentence_transformers import SentenceTransformer, util\n","from transformers import T5Tokenizer\n","from nltk.tokenize import sent_tokenize\n","\n","from tqdm import tqdm\n","import time\n","\n","EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","TOKENIZER_MODEL = 't5-small'\n","\n","\n","# Load models\n","model = SentenceTransformer(EMBEDDING_MODEL)  # Lightweight embedding model\n","tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_MODEL)  # Tokenizer for T5\n","\n","# Function to process each row\n","def select_sentences(row):\n","    sentences = sent_tokenize(row['sentences'])  # Split sentences by period and newlines\n","    catchphrases = row['catchphrases']\n","\n","    # Encode sentences and catchphrases\n","    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n","    phrase_embeddings = model.encode(catchphrases, convert_to_tensor=True)\n","\n","    # Compute cosine similarities\n","    similarities = util.cos_sim(phrase_embeddings, sentence_embeddings)\n","\n","    # Prepare to select useful sentences\n","    selected_sentences = []\n","    selected_tokens = 0\n","    max_token_limit = 500\n","    added_sentences = set()  # Track added sentences\n","\n","    # Rank sentences by similarity\n","    similarity_scores = [(sentences[i], max(similarities[:, i]).item()) for i in range(len(sentences))]\n","    sorted_sentences = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n","\n","    # Add sentences while staying within the token limit\n","    for sentence, score in sorted_sentences:\n","        if sentence in added_sentences:\n","            continue  # Skip duplicates\n","\n","        # Calculate token count for the sentence\n","        tokens = tokenizer(sentence, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n","        token_count = tokens.shape[1]\n","\n","        # Stop if the token limit is exceeded\n","        if selected_tokens + token_count > max_token_limit:\n","            break\n","\n","        # Add the sentence and update token count\n","        selected_sentences.append(sentence)\n","        selected_tokens += token_count\n","        added_sentences.add(sentence)\n","\n","    # Return the selected sentences as a single string\n","    return \" \".join(selected_sentences)\n","\n","\n","\n","def track_time(df, func, axis=1):\n","    tqdm.pandas(desc=\"Processing rows\")  # Initialize tqdm progress bar\n","    start_time = time.time()\n","\n","    # Apply the function with tqdm progress tracking\n","    result = df.progress_apply(func, axis=axis)\n","\n","    # Calculate total processing time\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","\n","    print(f\"Processing completed in {total_time:.2f} seconds.\")\n","    return result"]},{"cell_type":"code","source":["input_file_path = f\"{DRIVE_HOME}{CODE_HOME}/2_Generation/filtered_data/filtered_summ_data.csv\"\n","summ_data = pd.read_csv(input_file_path)\n","\n","# summ_data_samp = summ_data[:10].copy()\n","summ_data_samp = summ_data.copy()\n","\n","# Run Pipeline\n","summ_data_samp['selected_sentences'] = track_time(summ_data_samp, select_sentences)\n","\n","output_file_path = f\"{DRIVE_HOME}{CODE_HOME}/2_Generation/catchphrase_Extraction/preprocessed_summ_data.csv\"\n","summ_data_samp.to_csv(output_file_path, index=False)\n","print(summ_data_samp.shape, summ_data_samp.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFD7iShepsFC","outputId":"c8bf47bb-8b44-4348-8d4a-9d72998dbfd9","executionInfo":{"status":"ok","timestamp":1732402253105,"user_tz":300,"elapsed":305566,"user":{"displayName":"Nithish Kumar","userId":"11511991541260541160"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing rows: 100%|██████████| 2871/2871 [05:01<00:00,  9.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing completed in 301.83 seconds.\n","(2871, 9) Index(['filename', 'name', 'AustLII', 'catchphrases', 'sentences',\n","       'word_count', 'num_catchphrases', 'total_tokens', 'selected_sentences'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hs11svtipsDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G8eqQoUvpsA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GVe0EiiQpr-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qi6g_q2Tpr8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rr75dQ05pr6J"},"execution_count":null,"outputs":[]}]}